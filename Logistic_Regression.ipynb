{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHv5wfoyEcZTW5duLsw14m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranjithdurgunala/ML-LAB-2025-2026/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Logistic Regression?**\n",
        "\n",
        "Despite its name, Logistic Regression is a classification algorithm, not a regression one. It's a fundamental supervised learning method used to predict the probability that an input belongs to a specific category.\n",
        "\n",
        "\n",
        "Its most common use is for binary classification, where there are only two possible outcomes (e.g., Yes/No, 1/0, True/False, Spam/Not Spam).\n",
        "\n",
        "The core idea is to take a set of input features and output a probability value between 0 and 1. This probability is then used to make a final classification."
      ],
      "metadata": {
        "id": "X6Dn8CJp5I4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***How It Works:*** The Sigmoid FunctionLogistic Regression works in two main steps:Linear Step: First, just like linear regression, it calculates a weighted sum of the input features. This result is a single number that can be anything from negative infinity to positive infinity.6Equation: $z = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$Logistic (Sigmoid) Step: This is the key part.7 The **algorithm** passes the linear result (8$z$) through a special function called the Sigmoid function (or Logistic function).9 This S-shaped function \"squishes\" any real number into a range between 0 and 1.10Sigmoid Equation: 11$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$The output of this function is the probability that the input belongs to \"Class 1\".12If the output is 0.8, it means there's an 80% probability of it being Class 1.13If the output is 0.1, it means there's a 10% probability of it being Class 1.14Finally, a decision threshold (usually 0.5) is used to make the final call:15If Probability 16$\\geq$ 0.5, predict Class 1.17If Probability 18$<$ 0.5, predict Class 0.19"
      ],
      "metadata": {
        "id": "EVdYy3Z_5No6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yMwNs-84kiN",
        "outputId": "a80417e4-276c-4433-dd97-632586c33112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model Evaluation ---\n",
            "Accuracy: 0.9825 (or 98.25%)\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 62   1]\n",
            " [  2 106]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.97      0.98      0.98        63\n",
            "      benign       0.99      0.98      0.99       108\n",
            "\n",
            "    accuracy                           0.98       171\n",
            "   macro avg       0.98      0.98      0.98       171\n",
            "weighted avg       0.98      0.98      0.98       171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target # Target (0 = malignant, 1 = benign)\n",
        "\n",
        "# 2. Split the data\n",
        "# We'll use 70% for training and 30% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Scale the features\n",
        "# Logistic regression (like many algorithms) performs better when features are scaled.\n",
        "# We fit the scaler on the training data and use it to transform both sets.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Create and Train the Logistic Regression Model\n",
        "# We instantiate the model and then 'fit' it to our training data.\n",
        "# 'solver='lbfgs'' is a common default. 'max_iter' might be needed for convergence.\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make Predictions\n",
        "# Use the trained model to predict the classes for the unseen test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Evaluate the Model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"--- Model Evaluation ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f} (or {accuracy*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ]
    }
  ]
}