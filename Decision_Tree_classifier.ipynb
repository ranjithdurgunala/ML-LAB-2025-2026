{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJbBVPJ0K4DJxc2O0GqZa4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranjithdurgunala/ML-LAB-2025-2026/blob/main/Decision_Tree_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Decision Tree Classifier**\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It's one of the most intuitive and easy-to-understand models.\n",
        "\n",
        "Think of it like a flowchart or a game of \"20 Questions.\" The tree starts with a single root node representing the entire dataset. It then splits the data into smaller, more homogeneous groups based on a series of questions about the features. Each question corresponds to an internal node, and the possible answers are the branches. The process continues until it reaches the leaf nodes, which represent the final classification or prediction.\n",
        "\n",
        "The goal during training is to find the best questions (splits) that most effectively separate the data into pure classes. To do this, it commonly uses metrics like the Gini Impurity or Entropy to measure the \"disorder\" or \"impurity\" of a node. A lower impurity means the data in that node is more uniform (i.e., belongs to a single class).\n",
        "\n",
        "Key Advantages:\n",
        "\n",
        "Easy to interpret: Its flowchart-like structure is simple to visualize and explain.\n",
        "\n",
        "Handles both numerical and categorical data: It can work with different types of features.\n",
        "\n",
        "Non-parametric: It doesn't make strong assumptions about the underlying distribution of the data.\n",
        "\n",
        "Key Disadvantage:\n",
        "\n",
        "Prone to overfitting: Without constraints (like limiting the tree's depth), it can create overly complex trees that memorize the training data but fail to generalize to new data. This is why hyperparameter tuning is crucial.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rbQ8XvxUS8uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scikit-learn (sklearn)**\n",
        "\n",
        "Scikit-learn is a comprehensive, open-source machine learning library. It provides simple and efficient tools for data mining and data analysis.\n",
        "\n",
        "sklearn.datasets: This module provides helper functions to load popular datasets, like the wine dataset (load_wine) used in the example. These built-in datasets are great for practicing and testing algorithms.\n",
        "\n",
        "sklearn.model_selection: This module contains essential tools for splitting data and tuning models.\n",
        "\n",
        "train_test_split: A function to randomly split a dataset into training and testing subsets. This is fundamental for evaluating a model's performance on unseen data.\n",
        "\n",
        "GridSearchCV: An automated tool for hyperparameter tuning. It exhaustively searches through a specified grid of parameters and uses cross-validation to find the combination that yields the best performance.\n",
        "\n",
        "sklearn.tree: This module contains the decision tree-based models.\n",
        "\n",
        "DecisionTreeClassifier: The class that implements the Decision Tree algorithm for classification tasks.\n",
        "\n",
        "sklearn.metrics: This module provides tools for evaluating model performance.\n",
        "\n",
        "accuracy_score: A function that calculates the accuracy of a classification model, which is the proportion of correct predictions."
      ],
      "metadata": {
        "id": "FHnwDYx_TiCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Basic Decision Tree Implementation***\n",
        "\n",
        "First, let's build and train a simple Decision Tree classifier with its default settings."
      ],
      "metadata": {
        "id": "TC3l5CGITtxi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msvgvfqLRmk6",
        "outputId": "52a60b0f-78a6-45ad-b7fa-c7bcbee3d8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of default Decision Tree: 0.9444\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "# The wine dataset is great for classification examples.\n",
        "wine = load_wine()\n",
        "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "y = pd.Series(wine.target)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "# 80% for training, 20% for testing.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree model\n",
        "# Using default parameters first.\n",
        "dt_default = DecisionTreeClassifier(random_state=42)\n",
        "dt_default.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred_default = dt_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "print(f\"Accuracy of default Decision Tree: {accuracy_default:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2. Hyperparameter Tuning with GridSearchCV***\n",
        "\n",
        "Hyperparameter tuning is the process of finding the optimal model architecture. For a Decision Tree, key parameters to tune include:\n",
        "\n",
        "criterion: The function to measure the quality of a split ('gini' or 'entropy').\n",
        "\n",
        "max_depth: The maximum depth of the tree. Limiting this helps prevent overfitting.\n",
        "\n",
        "min_samples_split: The minimum number of samples required to split an internal node.\n",
        "\n",
        "min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
        "\n",
        "We will use GridSearchCV to test a \"grid\" of different parameter combinations and find the best one using cross-validation."
      ],
      "metadata": {
        "id": "u82b-kR8S15x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 1. Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 5, 10, 15, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# 2. Set up GridSearchCV\n",
        "# The estimator is our Decision Tree model.\n",
        "# cv=5 means 5-fold cross-validation.\n",
        "# n_jobs=-1 uses all available CPU cores to speed up the process.\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
        "\n",
        "# 3. Fit GridSearchCV to the training data\n",
        "# This will train the model with every parameter combination.\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 4. Get the best parameters and the best model\n",
        "print(\"\\nBest Parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "best_dt = grid_search.best_estimator_\n",
        "\n",
        "# 5. Evaluate the tuned model\n",
        "y_pred_tuned = best_dt.predict(X_test)\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "\n",
        "print(f\"\\nAccuracy of default Decision Tree: {accuracy_default:.4f}\")\n",
        "print(f\"Accuracy of tuned Decision Tree:   {accuracy_tuned:.4f} ✨\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SjtmaU7S2RI",
        "outputId": "fb5fe743-0645-4f1b-b5c2-b9dceb58cfb9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
            "\n",
            "Best Parameters found by GridSearchCV:\n",
            "{'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
            "\n",
            "Accuracy of default Decision Tree: 0.9444\n",
            "Accuracy of tuned Decision Tree:   0.9444 ✨\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explanation of the Tuning Process***\n",
        "\n",
        "Define Parameter Grid: We create a dictionary (param_grid) where keys are the parameter names (max_depth, etc.) and values are lists of settings to try for those parameters.\n",
        "\n",
        "Instantiate GridSearchCV: We pass our base model (dt), the parameter grid, and a cross-validation strategy (cv=5). GridSearchCV will now train and evaluate the model for every single combination of parameters in the grid. For example, it will test a tree with criterion='gini', max_depth=5, min_samples_split=2, min_samples_leaf=1, and so on.\n",
        "\n",
        "Fit: Calling .fit() starts this exhaustive search process on the training data.\n",
        "\n",
        "Best Estimator: After the search is complete, grid_search.best_params_ gives you the combination that performed the best, and grid_search.best_estimator_ gives you the model already trained with these optimal parameters.\n",
        "\n",
        "Final Evaluation: By comparing the accuracy of the default model and the tuned model on the test set, you can see the improvement gained from hyperparameter tuning. Typically, the tuned model has better (or at least more reliable) performance."
      ],
      "metadata": {
        "id": "oZuYbYbAT5z6"
      }
    }
  ]
}